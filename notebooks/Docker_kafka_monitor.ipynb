{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# This will load play data into a Kafka instance. Only run once as otherwise the topics won't be ordered."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To start Kafka 9 in Docker (using image from https://hub.docker.com/r/flozano/kafka/) run the following in the Docker bash:\n",
    "\n",
    "```\n",
    "docker run -p 2181:2181 -p 9092:9092 --env _KAFKA_advertised_host_name=`docker-machine ip \\`docker-machine active\\`` --env _KAFKA_advertised_port=9092 --name=local_kafka flozano/kafka &\n",
    "```\n",
    "\n",
    "To kill it run  \n",
    "```\n",
    "docker kill local_kafka\n",
    "```\n",
    "then \n",
    "```\n",
    "docker rm local_kafka\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafka:9092\n"
     ]
    }
   ],
   "source": [
    "# require at least Python 3.5 for async/await to work\n",
    "import sys\n",
    "ver=sys.version_info\n",
    "assert (ver[0]>=3 and ver[1]>=5)\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../../msq-domain/src')\n",
    "\n",
    "import aiostreams.operators as op\n",
    "from aiostreams.config import QCConfigProvider\n",
    "\n",
    "#QCConfigProvider().kafka_broker = '0.0.0.0:9092'\n",
    "kafka_host = QCConfigProvider().kafka_broker\n",
    "# !set _KAFKA_advertised_host_name=0.0.0.0\n",
    "# !set _KAFKA_advertised_port=9092\n",
    "print(kafka_host)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try writing just one record to Kafka using the basic kafka library, \n",
    "# and reading back from it, to make sure Kafka is up, \n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import pickle\n",
    "\n",
    "topic='test_topic_2'\n",
    "\n",
    "kprod=KafkaProducer(bootstrap_servers=kafka_host)\n",
    "kprod.send(topic,pickle.dumps('test'))\n",
    "\n",
    "consumer = KafkaConsumer(topic,bootstrap_servers=kafka_host,auto_offset_reset='earliest',\n",
    "                         group_id=None, value_deserializer=lambda x: pickle.loads(x))\n",
    "next(consumer).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n",
      "2017-09-04 05:30:01.933000\n"
     ]
    }
   ],
   "source": [
    "# let's check the dump was successful\n",
    "from aiostreams import AsyncKafkaSource, run\n",
    "import aiostreams.operators as op\n",
    "from mosaicsmartdata.common.json_convertor import *\n",
    "import json\n",
    "\n",
    "input_graph = AsyncKafkaSource('bond-trades-topic', bootstrap_servers=kafka_host ,\\\n",
    "                               value_deserializer =  lambda x: x.decode('utf-8'))|\\\n",
    "                           op.map(lambda x: json_to_domain(x.value).timestamp) > print # op.map(lambda x: json.loads(x.value) ) > print# \n",
    "                #\n",
    "                \n",
    "run(input_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n",
      "2017-09-04 03:30:01.933000\n"
     ]
    }
   ],
   "source": [
    "# let's check the dump was successful\n",
    "from aiostreams import AsyncKafkaSource, run\n",
    "import aiostreams.operators as op\n",
    "from mosaicsmartdata.common.json_convertor import *\n",
    "import json\n",
    "\n",
    "input_graph = AsyncKafkaSource('bond-quotes-topic', bootstrap_servers=kafka_host,\\\n",
    "                               value_deserializer =  lambda x: x.decode('utf-8')) |\\\n",
    "                                op.map(lambda x: json_to_domain(x.value).timestamp) > print# #op.map(lambda x: json.loads(x.value) ) > []\n",
    "                           #\n",
    "                            #\n",
    "                #\n",
    "                \n",
    "run(input_graph)\n",
    "# print(input_graph.sink[0].value)\n",
    "# json_to_domain(input_graph.sink[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "# let's check the dump was successful\n",
    "from aiostreams import AsyncKafkaSource, run\n",
    "import aiostreams.operators as op\n",
    "from mosaicsmartdata.common.json_convertor import *\n",
    "import json\n",
    "\n",
    "input_graph = AsyncKafkaSource('output-topic', bootstrap_servers=kafka_host,\\\n",
    "                               value_deserializer =  lambda x: x.decode('utf-8')) >print\n",
    "                #op.map(lambda x: json_to_domain(x.value)) > print\n",
    "                #op.map(lambda x: json.loads(x.value) ) > print\n",
    "                \n",
    "run(input_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# now let's feed that data into our nice Kafka wrapper: first eur quotes\n",
    "from aiostreams.kafka import AsyncKafkaPublisher\n",
    "\n",
    "print(len(eurquotes))\n",
    "\n",
    "pub1 = AsyncKafkaPublisher(bootstrap_servers=kafka_host, topic='eurquotes', value_serializer = 'json')\n",
    "#pub1.verbose=True\n",
    "to_async_iterable(eurquotes) > pub1\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and then gbp quotes\n",
    "print(len(eurgbpquotes))\n",
    "\n",
    "pub1.topic='eurgbpquotes'\n",
    "to_async_iterable(eurgbpquotes) > pub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# and now let's publish trades to a joint topic, just for the fun of it\n",
    "\n",
    "eurtrades = to_async_iterable(eurtrades)\n",
    "eurgbptrades = to_async_iterable(eurgbptrades)\n",
    "joint_trade_stream = op.merge_sorted([eurgbptrades,eurtrades], lambda x: x.timestamp)\n",
    "\n",
    "pub1.topic = 'trades'\n",
    "joint_trade_stream > pub1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
