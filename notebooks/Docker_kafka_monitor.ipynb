{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "kafka:9092\n"
     ]
    }
   ],
   "source": [
    "# require at least Python 3.5 for async/await to work\n",
    "import sys\n",
    "ver=sys.version_info\n",
    "assert (ver[0]>=3 and ver[1]>=5)\n",
    "sys.path.append('../src')\n",
    "sys.path.append('../../msq-domain/src')\n",
    "sys.path.append('../../quant_container/src')\n",
    "\n",
    "import aiostreams.operators as op\n",
    "from aiostreams.config import QCConfigProvider\n",
    "# this is where AsyncKafkaSource, AsyncKafkaPublisher get the Kafka broker location\n",
    "QCConfigProvider().kafka_broker = 'kafka:9092'\n",
    "\n",
    "print(QCConfigProvider().kafka_broker)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'test'"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# try writing just one record to Kafka using the basic kafka library, \n",
    "# and reading back from it, to make sure Kafka is up, \n",
    "from kafka import KafkaProducer, KafkaConsumer\n",
    "import pickle\n",
    "\n",
    "topic='test_topic_2'\n",
    "kafka_host = QCConfigProvider().kafka_broker\n",
    "\n",
    "kprod=KafkaProducer(bootstrap_servers=kafka_host)\n",
    "kprod.send(topic,pickle.dumps('test'))\n",
    "\n",
    "consumer = KafkaConsumer(topic,bootstrap_servers=kafka_host,auto_offset_reset='earliest',\n",
    "                         group_id=None, value_deserializer=lambda x: pickle.loads(x))\n",
    "next(consumer).value"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the dump was successful\n",
    "from aiostreams import AsyncKafkaSource, run\n",
    "import aiostreams.operators as op\n",
    "from mosaicsmartdata.common.json_convertor import *\n",
    "import json\n",
    "\n",
    "input_graph = AsyncKafkaSource('bond-quotes-topic', \\\n",
    "                               value_deserializer =  lambda x: x.decode('utf-8'))|\\\n",
    "                            op.map(lambda x: json_to_domain(x.value).timestamp) > print # \n",
    "                           #op.map(lambda x: json.loads(x.value)['marketDataSnapshotFullRefreshList'][0]['timestamp']) > print #op.map(lambda x: json_to_domain(x.value).timestamp) > print #op.map(lambda x: x.value ) > print\n",
    "                           # op.map(lambda x: json.loads(x.value) ) > print# \n",
    "                           #op.map(lambda x: json.loads(x.value) ) > print# \n",
    "                #\n",
    "run(input_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# let's check the dump was successful\n",
    "from aiostreams import AsyncKafkaSource, run\n",
    "import aiostreams.operators as op\n",
    "from mosaicsmartdata.common.json_convertor import *\n",
    "import json\n",
    "\n",
    "input_graph = AsyncKafkaSource('bond-trades-topic', \\\n",
    "                               value_deserializer =  lambda x: x.decode('utf-8'))| \\\n",
    "                                op.map(lambda x: json_to_domain(x.value).timestamp) > print#\n",
    "                               #op.map(lambda x: print(x.value) > print# #op.map(lambda x: json.loads(x.value) ) > []                            \n",
    "    # #op.map(lambda x: json.loads(x.value) ) > []\n",
    "                           #\n",
    "                            #\n",
    "                #\n",
    "                \n",
    "run(input_graph)\n",
    "# print(input_graph.sink[0].value)\n",
    "# json_to_domain(input_graph.sink[0].value)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Look at what QC is outputting inside docker-compose\n",
    "from aiostreams import AsyncKafkaSource, run\n",
    "import aiostreams.operators as op\n",
    "from mosaicsmartdata.common.json_convertor import *\n",
    "import json\n",
    "\n",
    "input_graph = AsyncKafkaSource('output-topic',\\\n",
    "                               value_deserializer =  lambda x: x.decode('utf-8'))|\\\n",
    "                op.map(lambda x: x.value ) > print\n",
    "                #op.map(lambda x: json_to_domain(x.value)) > print\n",
    "                #op.map(lambda x: json.loads(x.value) ) > print\n",
    "                \n",
    "run(input_graph)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'persistence_policy'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-16-ffa5606c5948>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      9\u001b[0m \u001b[0mp\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mKafkaPersister\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     10\u001b[0m \u001b[0mreflated_graph\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mp\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mload\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mgraph_name\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 11\u001b[1;33m \u001b[0mreflated_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpersistence_policy\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m \u001b[1;31m# don't want to persist states when we re-run\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     12\u001b[0m \u001b[0mreflated_graph\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msink\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mprint\u001b[0m \u001b[1;31m#so we don't pollute the original output topic\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'persistence_policy'"
     ]
    }
   ],
   "source": [
    "# Let's load a snapshot of the graph locally and run against the same data\n",
    "from aiostreams import AsyncKafkaSource, AsyncKafkaPublisher, run\n",
    "import aiostreams.operators as op\n",
    "from aiostreams.persist import KafkaPersister\n",
    "from mosaicsmartdata.common.json_convertor import *\n",
    "import json\n",
    "\n",
    "graph_name = 'test_pid__markouts_unhedged' # got that name from the logs\n",
    "p = KafkaPersister()\n",
    "reflated_graph = p.load(graph_name)\n",
    "reflated_graph.persistence_policy = None # don't want to persist states when we re-run\n",
    "reflated_graph.sink = print #so we don't pollute the original output topic\n",
    "                \n",
    "run(reflated_graph)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
